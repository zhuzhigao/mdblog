<!--
title: 机器学习小结
date: 2017-05-27 09:00:13
tags:
- Machine Learning
- Data Analysis
- K-Means
- KNN
- Bayes
- Decision Tree
- Support Vector Machine
- Neutual Network
- Regression
-->
### 机器学习小结
Date: 2017-05-27 09:00:13
# 机器学习
公司需要做一个机器学习的活动，作为组织者之一参与了题目的测试，很有意思。也乘机开始学习一些机器学习的算法 - 机器学习是一个趋势，不仅仅是在互联网。讲道理这需要大学的很多数学与统计学的知识，可惜已经忘得差不多了。当年咋并不知道这么有用呢？- 对不起老师的有一个例证。跟着YouTube学习了一系列[教程](https://www.youtube.com/watch?v=kjhiXQfaFeo&list=PLO5e_-yXpYLARtW5NPHTFVYY-xpgwuNNH&index=1)，算是比较系统的了解了常用算法,总结一下,也简单复习一下。
<!-- more -->

![image](/images/ml/overview.png)

BTW, 机器学习，算法基本上都比较简单，最难的是数学建模，把那些业务中的特性抽象成向量的过程，另一个是选取适合模型的数据样本。这两个事都不是简单的事。算法反而是比较简单的事。

## 归类 vs 回归
归类跟回归是两大类机器学习算法。其实本质上并没有那么的大的区别。简单来说，归类是针对的离散数值，回归是针对的连续数值。但是都需要建模，都可以用来进行分析，归类，预测，异常分析，推荐等常见的应用。

## 监督 vs 非监督
监督算法输入数据已经知道输出结果（如分类结果），而非监督算法需要根据数据预测，算法根据数据的规则（如分布）对数据进行分类和学习。监督算法相对较多也较常用。介于两者之间的称为半监督。

## 监督学习

### 决策树 (Decision Tree)
根据墒的下降来构建决策树。计算总墒，以及在每一个条件确定的情况下的墒，墒下降对快的条件就作为决策树的根节点。之后递归同样的操作（多个条件确定的情况下求墒下降最快的条件），知道所有条件被遍历或者所有子节点的分类一样。注意同一个属性的不同判断条件可以出现在树的不同层。
墒：H(x)= -∑p(xi)log(p(xi))(i=1,2,..n)。
信息增益：IG(T)=H(X) - H(X|T)

决策树容易产生过拟合，通过限制树高或者后剪枝来避免过拟合。

常见的算法包括属性选择度量方法不同： C4.5 （gain ratio), CART(gini index), ID3 (Information Gain), 随机森林 (Random Forest) 等。随机森林根据实例在同一个节点中出现的次数（实例间距离）还可以进行无监督学习/分类。

优点：直观，便于理解，小规模数据集有效。

缺点：处理连续变量不好；类别较多时，错误增加的比较快； 可规模性一般。

适用数据范围：数值型和标称型

### 临近取样 (KNN: K Nearest Neighbor)
临近取样是一个相对简单直观的算法。对于某个需要分类点，计算到所有点的距离，从中挑出k个最近的点，按照这些点的类别确定该点的类别，最简单的方法就是多数原则。
![image](/images/ml/knn/distance.png)

knn还可以有一些其他应用。如手写识别中使用knn求到标准数字的最小距离来确定识别的数字；平滑曲线时，使用选择最近的k个点，求平均值，作为插值,等。

特点：k值的选择会对k近邻法的结果产生重大影响

优点：精度高、对异常值不敏感、无数据输入假定

缺点：计算复杂度高、空间复杂度高

适用数据范围：数值型和标称型

### 朴素贝叶斯
另一个比较直观的算法，简单来说就是比较某个实例的特征在各个分类中的概率乘积，乘积最大的类就是该实例的分类。实际中使用ln之和来替代概率乘积，防止乘积过小或者某个概率为0。

理论：![image](/images/ml/bayes/concept.jpg)

方法：![image](/images/ml/bayes/howto.jpg)

优点：学习和预测的效率高，且易于实现；在数据较少的情况下仍然有效，可以处理多类别问题。

缺点：分类性能不一定很高；条件假设独立会使朴素贝叶斯变得简单，但有时会牺牲一定的分类准确率。

适用数据范围：标称型数据

### 支持向量机 (SVM: Supported Vector Machine)
在神经网络前，SVM被认为机器学习表现最好的算法。寻找区分两类的超平面（hyper plane), 使边际(margin)最大。为了便于推导，使用(-1, 1)分类 - 相较于逻辑回归的(0, 1)分类。

logistic函数:![image](/images/ml/svm/lr.png)

公式：

![image](/images/ml/svm/formula.jpg)

![image](/images/ml/svm/formula2.jpg)

最大边际的超平面(MMH)推导很复杂，参见[http://taop.marchtea.com/07.02.svm.html](http://taop.marchtea.com/07.02.svm.html).求解使用拉格朗日算子。

线性可区分(linear separable) 和 线性不可区分 （linear inseparable) 。对于线性不可区分，通过增加维度达到线性可区分。e.g, 对(x1, x2, x3), 转换为 (x1,x2,x3, x1x1,x1x2, x1x3)的六维空间。这个超平面是线性的，求解后带入回原方程。

所有坐落在边际的两边的的超平面上的被称作”支持向量(support vectors)"
- 训练好的模型的算法复杂度是由支持向量的个数决定的，而不是由数据的维度决定的。所以SVM不太容易产生overfitting
- SVM训练出来的模型完全依赖于支持向量(Support Vectors), 即使训练集里面所有非支持向量的点都被去除，重复训练过程，结果仍然会得到完全一样的模型。
- 一个SVM如果训练得出的支持向量个数比较小，SVM训练出的模型比较容易被泛化。

核方法（kernel method)在线性SVM中转化为最优化问题时求解的公式计算都是以内积(dot product)的形式出现的，因为内积的算法复杂度非常大，所以我们利用核函数来取代计算非线性映射函数的内积。核函数和非线性映射函数的内积等.
 
SVM扩展可解决多个类别分类问题。 one vs rest (n SVM, choose maximum), one vs one (n*(n-1)/2 SVM, choose majority), etc.

优点:
- 可以解决高维问题，即大型特征空间；
- 能够处理非线性特征的相互作用；
- 无需依赖整个数据；
- 可以提高泛化能力；

缺点:
- 当观测样本很多时，效率并不是很高；
- 对非线性问题没有通用解决方案，有时候很难找到一个合适的核函数；
- 对缺失数据敏感；

适用数据范围：数值型和标称型

### 神经网络 (NN: Neutual Network)
最著名的算法是反向传播法backpropagation.

![image](/images/ml/nn/formula.png)

![image](/images/ml/nn/layers.png)

![image](/images/ml/nn/compute.png)

-使用在多层向前神经网络上，由以下部分组成：输入层(input layer), 隐藏层 (hidden layers), 输入层 (output layers)
- 输入层(input layer)是由训练集的实例特征向量传入
- 经过连接结点的权重(weight)传入下一层，一层的输出是下一层的输入
- 隐藏层的个数可以是任意的，输入层有一层，输出层有一层
- 每个单元(unit)也可以被称作神经结点，根据生物学来源定义
- 一层中加权的求和，然后根据非线性方程转化输出：![image](/images/ml/nn/output.png)
- 作为多层向前神经网络，理论上，如果有足够多的隐藏层(hidden layers) 和足够大的训练集, 可以模拟出任何方程

初始化权重(weights)和偏向(bias):随机初始化在-1到1之间，或者-0.5到0.5之间. 每个节点一个偏向 根据误差(error)反向传送。
- 对于输出层：![image](/images/ml/nn/err1.png)
- 对于隐藏层：![image](/images/ml/nn/err2.png)
- 权重更新：  ![image](/images/ml/nn/update1.png)
- 偏向更新    ![image](/images/ml/nn/update2.png)
                                
神经网络可以用于分类和回归。对多分类，类别数K(K≥3)时，输出层需K个神经元；K＝2时，输出层只需一个神经元。预测时，输入样本属于输出“概率”最大的类别。

交叉验证方法(Cross-Validation)：将数据分成n份，依次其中部分作为测试集，其余作为训练集，来修正模型 （通常为求均值）。

优点：
- 分类准确度高，学习能力极强。
- 对噪声数据鲁棒性和容错性较强。
- 有联想能力，能逼近任意非线性关系。
缺点：
- 神经网络参数较多，权值和阈值。
- 黑盒过程，不能观察中间结果。
- 学习过程比较长，有可能陷入局部极小值

适用数据范围：数值型和标称型

### 线性回归 (LR: Linear Regression)
简单现行回归模型：y = w'x+e，e为误差服从正态分布。相当于求一个曲面，使得 SigMa(square(y - expectedy)) 最小
计算方法：![image](/images/ml/lr/compute.png)

多元线型回归模型：y=β0＋β１x1+β2x2+ ... +βpxp+ε, 多元线性回归的基本原理和基本计算过程与一元线性回归相同，但由于自变量个数多，计算相当麻烦，相当于求一个曲面，使得 SigMa(square(y - expectedy)) 最小。一般通过普通最小二乘法。![image](/images/ml/lr/compute2.png)

优点：结果易于理解，计算上不复杂。 

缺点：对非线性数据拟合不好。 

适用数据类型：数值型和标称型数据。 

### 逻辑回归 (LR: Logistic Regression)
logistic回归本质上是线性回归，只是在特征到结果的映射中加入了一层函数映射可以将连续值映射到0和1上。。对二分类逻辑回归。使用sigmoid函数来模拟单位阶跃函数。在0附近快速跃迁。对特征值乘以回归系数并求和，对和计算sigmoid，并以0.5为界划分类别。问题被简化为求最佳回归系数。通过梯度上升或下降法来改变回归系数，达到系数的最终收敛，使得损失(cost)最小。

SigMoid函数：
![image](/images/ml/svm/lr.png)

公式：
![image](/images/ml/lgr/formula.jpg)

Cost函数:
![image](/images/ml/lgr/cost.jpg)

求解公式 - 梯度下降法:
![image](/images/ml/lgr/compute.jpg)

当数据量大时，使用随机梯度上升/下降，在样本到来时对分类器进行增量式更新。可以通过根据迭代次数降低步长alpha来防止回归系数的大幅波动。

优点：计算代价不高，易于理解和实现。

缺点：容易欠拟合，分类的精度可能不高。

适用数据范围：数值型和标称型

### 相关度和R平方值
皮尔逊相关系数 (Pearson Correlation Coefficient):
- 衡量两个值线性相关强度的量
- 取值范围 [-1, 1]: 正向相关: >0, 负向相关：<0, 无相关性：=0

公式：![image](/images/ml/r2/cov.png)

R平方值: 决定系数，反应因变量的全部变异能通过回归关系被自变量解释的比例。如R平方为0.8，则表示回归关系可以解释因变量80%的变异。换句话说，如果我们能控制自变量不变，则因变量的变异程度会减少80%

公式：![image](/images/ml/r2/r2.jpg)

## 非监督学习
### K-Means
在K-means中，我们将中心点取为当前cluster中所有数据点的平均值

变种K-Medroid:从当前cluster中选取这样一个点——它到其他所有（当前cluster中的）点的距离之和最小——作为中心点

变种高斯混合模型 （GMM, Gauss Mixture Model)，K-means的进一步扩展，求每个点在不同的聚类下的高斯分布概率，所有点的乘积最大就是最合理的分布。算法开始执行时先对需要计算的参数赋初值，然后交替执行两个步骤，一个步骤是对数据的估计（k-means是估计每个点所属簇；GMM是计算隐含变量的期望；）;第二步是用上一步算出的估计值重新计算参数值，更新目标参数（k-means是计算簇心位置；GMM是计算各个高斯分布的中心位置和协方差矩阵）. [参见](http://blog.csdn.net/jwh_bupt/article/details/7663885).

优点：容易实现。

缺点：可能收敛到局部最小值，在大规模数据上收敛较慢。

适用数据范围：数值型数据。

### HC: Hierarchical Clustering
自底向顶的算法。假设有N个待聚类的样本，对于层次聚类来说，（初始化）把每个样本归为一类，计算每两个类之间的距离，也就是样本与样本之间的相似度；寻找各个类之间最近的两个类，把他们归为一类（这样类的总数就少了一个）；重新计算新生成的这个类与各个旧类之间的相似度；重复直到所有样本点都归为一类，结束

![image](/images/ml/hc/hc.png)

计算距离有不同的方法：最近距离，最远距离，平均距离，中值距离。

优点：距离和规则的相似度容易定义，限制少；不需要预先制定聚类数；可以发现类的层次关系；

缺点：计算复杂度太高；奇异值影响；可能聚类成链状；

适用数据范围：数值型数据。
